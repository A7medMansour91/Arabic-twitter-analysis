{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "\n",
      "def oauth_login():\n",
      "    #This code is taken from https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition \n",
      "    # XXX: Go to http://twitter.com/apps/new to create an app and get values\n",
      "    # for these credentials that you'll need to provide in place of these\n",
      "    # empty string values that are defined as placeholders.\n",
      "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
      "    # on Twitter's OAuth implementation.\n",
      "    \n",
      "    CONSUMER_KEY = ''\n",
      "    CONSUMER_SECRET = ''\n",
      "    OAUTH_TOKEN = ''\n",
      "    OAUTH_TOKEN_SECRET = ''\n",
      "    \n",
      "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
      "    \n",
      "    twitter_api = twitter.Twitter(auth=auth)\n",
      "    return twitter_api\n",
      "\n",
      "def twitter_search(twitter_api, q, max_results, **kw):\n",
      "\n",
      "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets and \n",
      "    # https://dev.twitter.com/docs/using-search for details on advanced \n",
      "    # search criteria that may be useful for keyword arguments\n",
      "    \n",
      "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
      "    search_results = twitter_api.search.tweets(q=q, count=10000, **kw)\n",
      "    \n",
      "    statuses = search_results['statuses']\n",
      "    print len(statuses)\n",
      "    \n",
      "    # Iterate through batches of results by following the cursor until we\n",
      "    # reach the desired number of results, keeping in mind that OAuth users\n",
      "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
      "    # https://dev.twitter.com/docs/rate-limiting/1.1/limits\n",
      "    # for details. A reasonable number of results is ~1000, although\n",
      "    # that number of results may not exist for all queries.\n",
      "    \n",
      "    # Enforce a reasonable limit\n",
      "    max_results = 10000 #min(100000, max_results)\n",
      "    \n",
      "    for _ in range(10000): # 10*100 = 1000\n",
      "        try:\n",
      "            next_results = search_results['search_metadata']['next_results']\n",
      "        except KeyError, e: # No more results when next_results doesn't exist\n",
      "          #  print len(statuses)\n",
      "            break\n",
      "            \n",
      "        # Create a dictionary from next_results, which has the following form:\n",
      "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
      "        kwargs = dict([ kv.split('=') \n",
      "                        for kv in next_results[1:].split(\"&\") ])\n",
      "        \n",
      "        print len(statuses)\n",
      "        search_results = twitter_api.search.tweets(q=q, count=10000, **kw)\n",
      "        statuses += search_results['statuses']\n",
      "        \n",
      "        if len(statuses) > max_results: \n",
      "            break\n",
      "    print len(statuses)\n",
      "            \n",
      "    return statuses"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import io, json\n",
      "\n",
      "def save_json( data):\n",
      "    with io.open('WAS.json', \n",
      "                 'w', encoding='utf-8') as f:\n",
      "        f.write(unicode(json.dumps(data, ensure_ascii=False)))\n",
      "\n",
      "def load_json(filename):\n",
      "    with io.open('WAS.json', \n",
      "                 encoding='utf-8') as f:\n",
      "        return f.read()\n",
      "\n",
      "# Sample usage\n",
      "\n",
      "#q = '%23%D8%A7%D9%8A%D8%AC%D8%A7%D8%A8%D9%8A%D8%A9'\n",
      "#twitter_api = oauth_login()\n",
      "#results = twitter_search(twitter_api, q, max_results=10000)\n",
      "\n",
      "#save_json(q, results)\n",
      "#results = load_json(q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import time\n",
      "from urllib2 import URLError\n",
      "from httplib import BadStatusLine\n",
      "import json\n",
      "import twitter\n",
      "\n",
      "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
      "    \n",
      "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
      "    # value for wait_period if the problem is a 500 level error. Block until the\n",
      "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
      "    # for 401 and 404 errors, which requires special handling by the caller.\n",
      "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
      "    \n",
      "        if wait_period > 3600: # Seconds\n",
      "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
      "            raise e\n",
      "    \n",
      "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
      "    \n",
      "        if e.e.code == 401:\n",
      "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
      "            return None\n",
      "        elif e.e.code == 404:\n",
      "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
      "            return None\n",
      "        elif e.e.code == 429: \n",
      "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
      "            if sleep_when_rate_limited:\n",
      "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
      "                sys.stderr.flush()\n",
      "                time.sleep(60*15 + 5)\n",
      "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
      "                return 2\n",
      "            else:\n",
      "                raise e # Caller must handle the rate limiting issue\n",
      "        elif e.e.code in (500, 502, 503, 504):\n",
      "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
      "                (e.e.code, wait_period)\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            return wait_period\n",
      "        else:\n",
      "            raise e\n",
      "\n",
      "    # End of nested helper function\n",
      "    \n",
      "    wait_period = 2 \n",
      "    error_count = 0 \n",
      "\n",
      "    while True:\n",
      "        try:\n",
      "            return twitter_api_func(*args, **kw)\n",
      "        except twitter.api.TwitterHTTPError, e:\n",
      "            error_count = 0 \n",
      "            wait_period = handle_twitter_http_error(e, wait_period)\n",
      "            if wait_period is None:\n",
      "                return\n",
      "        except URLError, e:\n",
      "            error_count += 1\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
      "                raise\n",
      "        except BadStatusLine, e:\n",
      "            error_count += 1\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
      "                raise\n",
      "\n",
      "def harvest_user_timeline(twitter_api, screen_name=None, user_id=None, max_results=3000):\n",
      "     \n",
      "    assert (screen_name != None) != (user_id != None), \\\n",
      "    \"Must have screen_name or user_id, but not both\"    \n",
      "    \n",
      "    kw = {  # Keyword args for the Twitter API call\n",
      "        'count': 2000,\n",
      "        'trim_user': 'true',\n",
      "        'include_rts' : 'true',\n",
      "        'since_id' : 1\n",
      "        }\n",
      "    \n",
      "    if screen_name:\n",
      "        kw['screen_name'] = screen_name\n",
      "    else:\n",
      "        kw['user_id'] = user_id\n",
      "        \n",
      "    max_pages = 16\n",
      "    results = []\n",
      "    \n",
      "    tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
      "    \n",
      "    if tweets is None: # 401 (Not Authorized) - Need to bail out on loop entry\n",
      "        tweets = []\n",
      "        \n",
      "    results += tweets\n",
      "    \n",
      "    print >> sys.stderr, 'Fetched %i tweets' % len(tweets)\n",
      "    \n",
      "    page_num = 1\n",
      "    \n",
      "    # Many Twitter accounts have fewer than 200 tweets so you don't want to enter\n",
      "    # the loop and waste a precious request if max_results = 200.\n",
      "    \n",
      "    # Note: Analogous optimizations could be applied inside the loop to try and \n",
      "    # save requests. e.g. Don't make a third request if you have 287 tweets out of \n",
      "    # a possible 400 tweets after your second request. Twitter does do some \n",
      "    # post-filtering on censored and deleted tweets out of batches of 'count', though,\n",
      "    # so you can't strictly check for the number of results being 200. You might get\n",
      "    # back 198, for example, and still have many more tweets to go. If you have the\n",
      "    # total number of tweets for an account (by GET /users/lookup/), then you could \n",
      "    # simply use this value as a guide.\n",
      "    \n",
      "    if max_results == kw['count']:\n",
      "        page_num = max_pages # Prevent loop entry\n",
      "    \n",
      "    while page_num < max_pages and len(tweets) > 0 and len(results) < max_results:\n",
      "    \n",
      "        # Necessary for traversing the timeline in Twitter's v1.1 API:\n",
      "        # get the next query's max-id parameter to pass in.\n",
      "        # See https://dev.twitter.com/docs/working-with-timelines.\n",
      "        kw['max_id'] = min([ tweet['id'] for tweet in tweets]) - 1 \n",
      "    \n",
      "        tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
      "        results += tweets\n",
      "\n",
      "        print >> sys.stderr, 'Fetched %i tweets' % (len(tweets),)\n",
      "    \n",
      "        page_num += 1\n",
      "        \n",
      "    print >> sys.stderr, 'Done fetching tweets'\n",
      "\n",
      "    return results[:max_results]\n",
      "    \n",
      "# Sample usage\n",
      "\n",
      "twitter_api = oauth_login()\n",
      "#tweets = harvest_user_timeline(twitter_api, user_id=\"@spagov\", max_results=3000)\n",
      "tweets = harvest_user_timeline(twitter_api,screen_name=\"spagov\", max_results=3000)\n",
      "save_json(tweets)\n",
      "# Save to MongoDB with save_to_mongo or a local file with save_json..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Fetched 200 tweets\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Done fetching tweets\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}